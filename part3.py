# -*- coding: utf-8 -*-
"""part3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bKktJE6-yon9h5P60PFSTDSI4kWYil5
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv("car_price_dataset.csv")


df = df[['Price', 'Year']].dropna()


X = df[['Year']]
y = df['Price']

#Split the dataset into training and testing sets (80/20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

#Print the results
print("Linear Regression Results:")
print(f"Model Coefficient (Slope): {model.coef_[0]:.2f}")
print(f"Model Intercept: {model.intercept_:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R² Score: {r2:.4f}")

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_test['Year'], y=y_test, label='Actual Prices')
sns.lineplot(x=X_test['Year'], y=y_pred, color='red', label='Predicted Prices')
plt.title('Simple Linear Regression: Price vs Year')
plt.xlabel('Year')
plt.ylabel('Car Price')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#Load dataset
df = pd.read_csv("car_price_dataset.csv")
df = df.dropna()

y = df['Price']

X = df.drop(['Price'], axis=1)

X = pd.get_dummies(X, drop_first=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Multiple Linear Regression Results:")
print(f"Mean Squared Error: {mse:,.2f}")
print(f"R² Score: {r2:.4f}")

plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Car Prices")
plt.grid(True)
plt.tight_layout()
plt.show()

#Load dataset
df = pd.read_csv("car_price_dataset.csv")

df = df.dropna()
# Convert 'Price' into binary classes: 'Low' and 'High' using median
df['Price_Class'] = pd.cut(df['Price'], bins=[-1, df['Price'].median(), df['Price'].max()], labels=['Low', 'High'])
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['Price_Class'])

X = df.drop(['Price', 'Price_Class'], axis=1)
X = pd.get_dummies(X, drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB(),
    "Decision Tree": DecisionTreeClassifier()
}


for name, model in models.items():
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    acc = accuracy_score(y_test, predictions)
    cm = confusion_matrix(y_test, predictions)

    print(f"\n {name}")
    print(f"Accuracy: {acc:.4f}")
    print("Confusion Matrix:")
    print(cm)

# Accuracy values
model_names = ["Logistic Regression", "KNN", "Naive Bayes", "Decision Tree"]
accuracies = [0.7505, 0.6505, 0.8510, 0.9090]

plt.figure(figsize=(8, 5))
plt.bar(model_names, accuracies, color=['steelblue', 'orange', 'green', 'purple'])
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.ylim(0.6, 1.0)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

df = pd.read_csv("car_price_dataset.csv")


df['Price_Class'] = pd.cut(df['Price'], bins=[-1, df['Price'].median(), df['Price'].max()], labels=['Low', 'High'])

# Display price ranges for Low and High categories
price_median = df['Price'].median()
price_max = df['Price'].max()

print(f"Price category ranges:")
print(f"Low  : Price <= {price_median}")
print(f"High : Price > {price_median} and <= {price_max}")

label_encoder = LabelEncoder()
df['Price_Class_Encoded'] = label_encoder.fit_transform(df['Price_Class'])


df_encoded = pd.get_dummies(df, columns=['Fuel_Type', 'Transmission'], drop_first=True)

X = df_encoded.drop(columns=['Price', 'Price_Class', 'Price_Class_Encoded', 'Brand', 'Model'])
y = df['Price_Class_Encoded']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

new_car = pd.DataFrame([{
    'Brand': 'Toyota',
    'Model': 'Corolla',
    'Year': 2018,
    'Engine_Size': 1.6,
    'Mileage': 45000,
    'Doors': 4,
    'Owner_Count': 2,
    'Fuel_Type': 'Petrol',
    'Transmission': 'Manual'
}])


new_car_encoded = pd.get_dummies(new_car)

new_car_encoded = new_car_encoded.reindex(columns=X.columns, fill_value=0)

# Predict class
predicted_class_encoded = model.predict(new_car_encoded)[0]
predicted_class = label_encoder.inverse_transform([predicted_class_encoded])[0]

print(f"The predicted price class for the new car is: {predicted_class}")

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage

df = pd.read_csv("car_price_dataset.csv")

cluster_data = df[['Price', 'Mileage', 'Engine_Size']].dropna()

scaler = StandardScaler()
scaled_data = scaler.fit_transform(cluster_data)

# ----- K-Means Clustering -----
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_data['KMeans_Cluster'] = kmeans.fit_predict(scaled_data)

# Plot K-Means Clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(x=cluster_data['Mileage'], y=cluster_data['Price'],
                hue=cluster_data['KMeans_Cluster'], palette='Set2')
plt.title('K-Means Clustering: Price vs Mileage')
plt.xlabel('Mileage')
plt.ylabel('Price')
plt.show()

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


df = pd.read_csv("car_price_dataset.csv")

features = ['Price', 'Mileage', 'Engine_Size']
df_cluster = df[features].dropna()


scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_cluster)

kmeans = KMeans(n_clusters=3, random_state=42)
df_cluster['Cluster'] = kmeans.fit_predict(scaled_features)

new_car = pd.DataFrame([{
    'Price': 0,
    'Mileage': 45000,
    'Engine_Size': 1.6
}])

# Scale the new car’s features
new_car_scaled = scaler.transform(new_car)
predicted_cluster = kmeans.predict(new_car_scaled)[0]
average_price = df_cluster[df_cluster['Cluster'] == predicted_cluster]['Price'].mean()
# Output
print(f"Predicted Cluster: {predicted_cluster}")
print(f"Estimated Price for the new car: {average_price:,.2f}")

# ----- Hierarchical Clustering -----
linked = linkage(scaled_data, method='ward')

plt.figure(figsize=(10, 6))
dendrogram(linked,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=False)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()